---
title: "An Introduction to Statistical Learning: Chapter 4 Applied Exercises"
author: "Kylie Foster"
date: "1 May 2019"
output: 
  prettydoc::html_pretty:
    theme: leonids
    highlight: github
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("ISLR")
library(MASS) # for lda, and contains a large collection of datasets
library(ISLR) # data sets for the ISLR book
library(GGally) # for ggpairs
library(tidyverse) # loads packages including ggplot2, dplyr
#library(skimr) # for nicer summaries using skim
#library(cowplot) # for plot_grid
#library(car) # for vif()
#library(olsrr) # for studentized residual plots
library(knitr) # for kable
library (class) # for knn()
library(ggmosaic) # for gg_mosaic
```

Still need to do Ex13

## Exercise 10

This question should be answered using the `Weekly` data set, which
is part of the `ISLR` package. This data is similar in nature to the
`Smarket` data from this chapterâ€™s lab, except that it contains 1,089
weekly returns for 21 years, from the beginning of 1990 to the end of
2010.

#### (a) Produce some numerical and graphical summaries of the `Weekly` data. Do there appear to be any patterns?

```{r 10_a, fig.height = 13, fig.width = 15, message = FALSE}
# taking an initial look at the data
glimpse(Weekly)

summary(Weekly)

# pairwise plots of all variables:
ggpairs(Weekly, mapping = aes(color = Direction)) +
  theme_bw()

# plot of all variables versus the predictor (direction)
theme_set(theme_bw()) # setting theme to black and white
Weekly_wrap <- gather(Weekly, -Direction, key = "vars", value = "values") # getting data 
# in the right format to use for facet_wrap. 
```

```{r 10_a_extra, message = FALSE, fig.height= 10}
ggplot(Weekly_wrap, aes(y = values, x = Direction)) +
    geom_boxplot() +
    facet_wrap(~ vars, scales = "free_y", ncol = 2)

```

The only strong correlation is between `Volume` and `Year` (0.84); all other correlations have an absolute value less than 0.1.

There are more days for which the market goes up than those for which it goes down.

The only clear relationship between `Direction` and the other variables is between `Direction` and `Today`, but this is not useful because it is just due to the definition of `Direction`.

#### (b) Use the full data set to perform a logistic regression with `Direction` as the response and the five lag variables plus `Volume` as predictors. Use the `summary` function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?

```{r 10_b}
log_model <- glm(Direction ~. -Today-Year, 
                 data = Weekly, family = binomial)

summary(log_model)

```

Only `Lag2` is statistically significant (p-value of `r round(coef(summary(log_model))[3,4], 2)`).

**(c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.**


```{r 10_c}
# Checking that `Up` is represented by 1 and `Down` is represented by 0: 
contrasts(Weekly$Direction)
# predicting probability of Up for entire dataset:
log_probs <- predict(log_model, Weekly, type = "response") 

# converting probabilities to class predictions:
glm_pred <- as.factor(case_when(log_probs > 0.5 ~ "Up", 
                             log_probs <= 0.5 ~ "Down"))

 # confusion matrix
kable(log_conf <- table(glm_pred, Weekly$Direction))

mean(glm_pred == Weekly$Direction) # overall fraction of correct predictions

# false positive rate (fraction of negative examples that are classified as positive )
log_conf[2,1]/(log_conf[1,1] + log_conf[2,1]) 
# false negative rate (fraction of positive examples that are classified as negative)
log_conf[1,2]/(log_conf[1,2] + log_conf[2,2]) 

```

If we naively assumed `Up` for all observations we would have an accuracy of `r round(100*mean("Up" == Weekly$Direction),2)`%, which is very similar to the accuracy of the logistic regression on the training set (56.1%), so the logistic regression model is not performing well even for the training set.

The logistic regression model is incorrectly classifying a lot of `Down` observations as `Up`, with a very high false positive rate of 89%. However, the model is relatively good at classifying `Up` observations correctly, with a low false negative rate of 7.9%. 

#### (d) Now fit the logistic regression model using a training data period from 1990 to 2008, with `Lag2` as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).

```{r 10_d}
train <- (Weekly$Year < 2009) # a Boolean vector (elements are TRUE or FALSE) the same length as 
# Year. The elements of the vector that correspond to observations that occurred before 2009 
# are set to TRUE, whereas those that correspond to observations after 2009 are set to FALSE

# fitting logistic with Lag2 as the only predictor, using just the data from 1990 to 2008:
log_model <- glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train) 

# predicting probability of Up for test set (not training set):
log_probs <- predict(log_model, Weekly[!train, ], type = "response") 

# converting probabilities to class predictions:
glm_pred <- as.factor(case_when(log_probs > 0.5 ~ "Up", 
                             log_probs <= 0.5 ~ "Down")) 

# confusion matrix:
kable(log_conf <- table(glm_pred, Weekly$Direction[!train])) 

mean(glm_pred == Weekly$Direction[!train]) # accuracy

```

The accuracy is improved (62.5% on the test set compared to 56.1% without using a test/train split).

#### (e) Repeat (d) using LDA.

```{r 10_e}
# fitting lda with Lag2 as the only predictor, using just the data from 1990 to 2008:
lda_model <- lda(Direction ~ Lag2, data = Weekly, subset = train) 

# predictions for test set (not training set): 
lda_class <- predict(lda_model, Weekly[!train, ])$class

# confusion matrix:
kable(lda_conf <- table(lda_class, Weekly$Direction[!train])) 

mean(lda_class == Weekly$Direction[!train]) # accuracy

```

The accuracy of the LDA model is the same as the accuracy of logistic regression model.

#### (f) Repeat (d) using QDA.

```{r 10_f}
# fitting qda with Lag2 as the only predictor, using just the data from 1990 to 2008:
qda_model <- qda(Direction ~ Lag2, data = Weekly, subset = train) 

# predictions for test set (not training set):
qda_class <- predict(qda_model, Weekly[!train, ])$class

# confusion matrix:
kable(qda_conf <- table(qda_class, Weekly$Direction[!train])) 

mean(qda_class == Weekly$Direction[!train]) # accuracy
```

The accuracy of QDA (58.7%) is lower compared to LDA and logistic regression (62.5% for both). QDA does not successfully predict any `Down` observations.

#### (g) Repeat (d) using KNN with K = 1.

```{r 10_g}
# no need to scale data because we are only using one predictor

# training set predictors. data.frame is required because knn interprets a 
# vector as a single observation of multiple predictors (with the number 
# of predictors equal to the length of the vector)
train_X <- data.frame(Weekly$Lag2[train]) 
test_X <- data.frame(Weekly$Lag2[!train]) # test set predictors
train_Direction <- Weekly$Direction[train] # class labels for training data

set.seed(1) # for reproducibility because if several observations are tied 
# as nearest neighbors then R will randomly break the tie.

knn_pred <- knn(train_X, test_X, train_Direction, k = 1)

# confusion matrix:
kable(knn_conf <- table(knn_pred, Weekly$Direction[!train])) 

mean(knn_pred == Weekly$Direction[!train]) # accuracy

```

KNN with K = 1 has the worst overall accuracy (50%).


#### (h) Which of these methods appears to provide the best results on this data?

Linear regression and LDA give the equal best overall accuracy (62.5%).

#### (i) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.

Accuracy to beat is 62.5%.

Trying KNN with `Lag2` as the only predictor with K values varying from 1:15:

```{r 10_i_knn}
# need to scale data 
standardized_Weekly <- as_tibble(scale(select(Weekly, -c(Direction)))) # should really do this using only the 
# test set max, min etc

# training set predictors. data.frame is required because knn interprets a 
# vector as a single observation of multiple predictors (with the number 
# of predictors equal to the length of the vector)
train_X <- data.frame(standardized_Weekly$Lag2[train]) 
test_X <- data.frame(standardized_Weekly$Lag2[!train]) # test set predictors
train_Direction <- Weekly$Direction[train] # class labels for training data

multiple_knn <- function(k_nn, train_X, test_X, train_Direction, Weekly){
  set.seed(1) # for reproducibility because if several observations are tied 
# as nearest neighbors then R will randomly break the tie.
  knn_pred <- knn(train_X, test_X, train_Direction, k = k_nn)

  acc <- mean(knn_pred == Weekly$Direction[!train]) # accuracy
  print(k_nn)
  print(acc)
  #plot(k_nn, acc, add=TRUE)
}

for (k_nn in 1:20) {
  multiple_knn(k_nn, train_X, test_X, train_Direction, Weekly)
}

# Need to find a way to nicely print the accuracy for all K values
```

None of these K values lead to accuracy higher than 62.5%

To explore other possible predictors, checking the accuracy and p-values for logistic regressions models containing each individual predictor:

```{r 10_i}
## Lag1
# fitting logistic regression with Lag1 as the only predictor:
log_model <- glm(Direction ~ Lag1, data = Weekly, family = binomial, subset = train) 
summary(log_model)
# predicting probability of Up for test set (not training set):
log_probs <- predict(log_model, Weekly[!train, ], type = "response") 
# converting probabilities to class predictions:
glm_pred <- as.factor(case_when(log_probs > 0.5 ~ "Up", 
                             log_probs <= 0.5 ~ "Down")) 
mean(glm_pred == Weekly$Direction[!train]) # accuracy

## We already know Lag2 results

## Lag3
# fitting logistic regression with Lag3 as the only predictor:
log_model <- glm(Direction ~ Lag3, data = Weekly, family = binomial, subset = train) 
summary(log_model)
# predicting probability of Up for test set (not training set):
log_probs <- predict(log_model, Weekly[!train, ], type = "response") 
# converting probabilities to class predictions:
glm_pred <- as.factor(case_when(log_probs > 0.5 ~ "Up", 
                             log_probs <= 0.5 ~ "Down")) 
# mean(glm_pred == Weekly$Direction[!train]) # accuracy
# all observations predicted as Up

## Lag4
# fitting logistic regression with Lag4 as the only predictor, using just the data from 1990 to 2008:
log_model <- glm(Direction ~ Lag4, data = Weekly, family = binomial, subset = train) 
summary(log_model)
# predicting probability of Up for test set (not training set):
log_probs <- predict(log_model, Weekly[!train, ], type = "response") 
# converting probabilities to class predictions:
glm_pred <- as.factor(case_when(log_probs > 0.5 ~ "Up", 
                             log_probs <= 0.5 ~ "Down")) 
# mean(glm_pred == Weekly$Direction[!train]) # accuracy
# all observations predicted as Up

## Lag5
# fitting logistic regression with Lag5 as the only predictor, using just the data from 1990 to 2008:
log_model <- glm(Direction ~ Lag5, data = Weekly, family = binomial, subset = train) 
summary(log_model)
# predicting probability of Up for test set (not training set):
log_probs <- predict(log_model, Weekly[!train, ], type = "response") 
# converting probabilities to class predictions:
glm_pred <- as.factor(case_when(log_probs > 0.5 ~ "Up", 
                             log_probs <= 0.5 ~ "Down")) 
mean(glm_pred == Weekly$Direction[!train]) # accuracy

## Volume
# fitting logistic regression with Volume as the only predictor, using just the data from 1990 to 2008:
log_model <- glm(Direction ~ Volume, data = Weekly, family = binomial, subset = train) 
summary(log_model)
# predicting probability of Up for test set (not training set):
log_probs <- predict(log_model, Weekly[!train, ], type = "response") 
# converting probabilities to class predictions:
glm_pred <- as.factor(case_when(log_probs > 0.5 ~ "Up", 
                             log_probs <= 0.5 ~ "Down")) 
mean(glm_pred == Weekly$Direction[!train]) # accuracy


```

Only the models with `Lag1` and `Lag2` as predictors had stastically significant predictors. Trying all algorithms with both `Lag1` and `Lag2` as predictors:

```{r 10_Lag1_Lag2}
## Lag1 and Lag2
# fitting logistic with Lag1 and Lag2 as predictors, using just the data from 1990 to 2008:
log_model <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial, subset = train) 
summary(log_model)
# predicting probability of Up for test set (not training set):
log_probs <- predict(log_model, Weekly[!train, ], type = "response") 
# converting probabilities to class predictions:
glm_pred <- as.factor(case_when(log_probs > 0.5 ~ "Up", 
                             log_probs <= 0.5 ~ "Down")) 
mean(glm_pred == Weekly$Direction[!train]) # accuracy

# fitting lda with Lag1 and Lag2 as the predictors, using just the data from 1990 to 2008:
lda_model <- lda(Direction ~ Lag1 + Lag2, data = Weekly, subset = train) 
# predictions for test set (not training set): 
lda_class <- predict(lda_model, Weekly[!train, ])$class
mean(lda_class == Weekly$Direction[!train]) # accuracy

# fitting qda with Lag 1 and Lag2 as the predictors, using just the data from 1990 to 2008:
qda_model <- qda(Direction ~ Lag1 + Lag2, data = Weekly, subset = train) 
# predictions for test set (not training set):
qda_class <- predict(qda_model, Weekly[!train, ])$class
mean(qda_class == Weekly$Direction[!train]) # accuracy

## Lag1 and Lag2
# training set predictors. data.frame is required because knn interprets a 
# vector as a single observation of multiple predictors (with the number 
# of predictors equal to the length of the vector)
train_X <- cbind(standardized_Weekly$Lag1, standardized_Weekly$Lag2)[train ,] 
test_X <- cbind (standardized_Weekly$Lag1, standardized_Weekly$Lag2)[!train ,] # test set predictors
train_Direction <- Weekly$Direction[train] # class labels for training data

for (k_nn in 1:20) {
  multiple_knn(k_nn, train_X, test_X, train_Direction, Weekly)
}

```

None of the models with both `Lag1` and `Lag2` had improved results.

Modelling all pair-wise interactions using each algorithm:

```{r 10_allint}

# fitting logistic with Lag1 and Lag2 as predictors, using just the data from 1990 to 2008:
log_model <- glm(Direction ~ (Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume)^2, data = Weekly, 
                 family = binomial, subset = train) 
# predicting probability of Up for test set (not training set):
log_probs <- predict(log_model, Weekly[!train, ], type = "response") 
# converting probabilities to class predictions:
glm_pred <- as.factor(case_when(log_probs > 0.5 ~ "Up", 
                             log_probs <= 0.5 ~ "Down")) 
mean(glm_pred == Weekly$Direction[!train]) # accuracy

## All interactions
# fitting lda with Lag1 and Lag2 as the predictors, using just the data from 1990 to 2008:
lda_model <- lda(Direction ~ (Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume)^2, data = Weekly, subset = train) 
# predictions for test set (not training set): 
lda_class <- predict(lda_model, Weekly[!train, ])$class
mean(lda_class == Weekly$Direction[!train]) # accuracy

## All interactions
# fitting qda with Lag1 and Lag2 as the predictors, using just the data from 1990 to 2008:
qda_model <- qda(Direction ~ (Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume)^2, data = Weekly, subset = train) 
# predictions for test set (not training set): 
qda_class <- predict(qda_model, Weekly[!train, ])$class
mean(qda_class == Weekly$Direction[!train]) # accuracy


## KNN all interactions
# creating interaction terms
Weekly_int <- mutate(Weekly, Lag12 = Lag1*Lag2, Lag13 = Lag1*Lag3, Lag14 = Lag1*Lag4, Lag15 = Lag1*Lag5, 
                     Lag1vol = Lag1*Volume,
                     Lag23 = Lag2*Lag3, Lag24 = Lag2*Lag4, Lag25 = Lag2*Lag5, Lag2vol = Lag2*Volume,
                     Lag34 = Lag3*Lag4, Lag35 = Lag3*Lag5, Lag3vol = Lag3*Volume,
                     Lag45 = Lag4*Lag5, Lag4vol = Lag4*Volume,
                     Lag5vol = Lag5*Volume)
# need to scale data 
standardized_Weekly <- as_tibble(scale(select(Weekly_int, -c(Direction, Today)))) # should really do 
# this using only the test set max, min etc
# training set predictors. data.frame is required because knn interprets a 
# vector as a single observation of multiple predictors (with the number 
# of predictors equal to the length of the vector)
train_X <- standardized_Weekly[train ,] 
test_X <- standardized_Weekly[!train ,] # test set predictors
train_Direction <- Weekly$Direction[train] # class labels for training data

multiple_knn <- function(k_nn, train_X, test_X, train_Direction, Weekly){
  set.seed(1) # for reproducibility because if several observations are tied 
# as nearest neighbors then R will randomly break the tie.
  knn_pred <- knn(train_X, test_X, train_Direction, k = k_nn)
  
  acc <- mean(knn_pred == Weekly$Direction[!train]) # accuracy
  print(k_nn)
  print(acc)
}

for (k_nn in 1:15) {
  multiple_knn(k_nn, train_X, test_X, train_Direction, Weekly)
}
```

Best model is still logsitic (or LDA) with `Lag2` as the only predictor.

## Exercise 11

**In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the `Auto` data set.**

#### (a) Create a binary variable, `mpg01`, that contains a 1 if `mpg` contains a value above its median, and a 0 if `mpg` contains a value below its median. You can compute the median using the `median()` function. Note you may find it helpful to use the `data.frame()` function to create a single data set containing both `mpg01` and the other `Auto` variables.

```{r 11_a}
# Creating a binary variable, `mpg01`, that contains a 1 if `mpg` contains a 
# value above its median, and a 0 if `mpg` contains a value below its median
Auto <- mutate(Auto, mpg01 = as.factor(case_when(mpg > median(mpg) ~ 1, 
                             mpg <= median(mpg) ~ 0))) 
```


#### (b) Explore the data graphically in order to investigate the association between `mpg01` and the other features. Which of the other features seem most likely to be useful in predicting `mpg01`? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.

```{r 11_b}
# getting data in the right format to use for facet_wrap. Excluding name and origin:
Auto_wrap <- gather(Auto, -c("mpg01", "name", "origin"), key = "vars", value = "values") 

# Using combined violin and boxplots to visualize possible relationships:
ggplot(Auto_wrap, aes(y = values, x = mpg01)) +
    geom_violin() +
    geom_boxplot(width=0.1) +
    facet_wrap(~ vars, scales = "free_y", ncol = 2)

```

```{r 11_b_factor}

ggplot(data = Auto) +
  geom_mosaic(aes(x = product(origin, mpg01), fill=mpg01)) +
  labs(x = "mpg01", y = "origin")

ggplot(data = Auto) +
  geom_mosaic(aes(x = product(cylinders, mpg01), fill=mpg01)) +
  labs(x = "mpg01", y = "cylinders")

```

All variables (except `name`) look useful for predicting `mgp01`.

#### (c) Split the data into a training set and a test set.

```{r 11_c}

# Set random seed, for reproducibility.
set.seed(10)

# create a list of random number ranging from 1 to number of rows from 
# actual data and 80% of the data into training data  
rand_index <- sort(sample(nrow(Auto), nrow(Auto)*0.8))

# creating training data set by selecting the output row values
train_Auto <- Auto[rand_index, ]

#creating test data set by not selecting the output row values
test_Auto <- Auto[-rand_index, ]

```

#### (d) Perform LDA on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained? 

```{r 11_d}
# Linear Discrimminant Analysis
# All variables as predictors except name and mpg
 # fitting lda with all predictors except name using training day
lda_model <- lda(mpg01 ~.-name - mpg, data = Auto, subset = rand_index)
lda_class <- predict(lda_model, test_Auto)$class # predictions for test set (not training set)
kable(lda_conf <- table(lda_class, test_Auto$mpg01)) # confusion matrix
mean(lda_class == test_Auto$mpg01) # test error

# Using only the least noisy predictors identified in (c)
lda_model <- lda(mpg01 ~ displacement + horsepower + origin + 
                   weight + cylinders, data = Auto, subset = rand_index) 

lda_class <- predict(lda_model, test_Auto)$class # predictions for test set (not training set)
kable(lda_conf <- table(lda_class, test_Auto$mpg01)) # confusion matrix
mean(lda_class == test_Auto$mpg01) # test error

```

Best test error accuracy for LDA is 91.1%

#### (e) Perform QDA on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained? 

```{r 11_e}
# Quadratic Discrimminant Analysis
# All variables as predictors except name and mpg
# fitting qda with all predictors except name using training day
qda_model <- qda(mpg01 ~.-name - mpg, data = Auto, subset = rand_index) 
qda_class <- predict(qda_model, test_Auto)$class # predictions for test set (not training set)
kable(qda_conf <- table(qda_class, test_Auto$mpg01)) # confusion matrix
mean(qda_class == test_Auto$mpg01) # test error

# Using only the least noisy predictors identified in (c)
qda_model <- qda(mpg01 ~ displacement + horsepower + origin + 
                   weight + cylinders, data = Auto, subset = rand_index) 

qda_class <- predict(qda_model, test_Auto)$class # predictions for test set (not training set)
kable(qda_conf <- table(qda_class, test_Auto$mpg01)) # confusion matrix
mean(qda_class == test_Auto$mpg01) # test error
```

Best QDA model has an accuracy of 91.1%.

#### (f) Perform logistic regression on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?

```{r 11_f}
# Set random seed, for reproducibility.
set.seed(10)

Auto_exc_name <- select(Auto, -name) # excluding name because for some reason including it
# causes difficulty with  predict 

# create a list of random number ranging from 1 to number of rows from 
# actual data and 80% of the data into training data  
rand_index <- sort(sample(nrow(Auto_exc_name), nrow(Auto_exc_name)*0.8))

# creating training data set by selecting the output row values
train_Auto <- Auto_exc_name[rand_index, ]

#creating test data set by not selecting the output row values
test_Auto <- Auto_exc_name[-rand_index, ]

# Logistic regression
# All variables as predictors except name
log_model <- glm(mpg01 ~. -mpg, data = Auto_exc_name, family = binomial, subset = rand_index) 
log_probs <- predict(log_model, test_Auto , type = "response") # predictions for test set (not training set)
log_class <- as.factor(case_when(log_probs > 0.5 ~ "1", 
                             log_probs <= 0.5 ~ "0")) # converting probabilities to class predictions
kable(log_conf <- table(log_class, test_Auto$mpg01)) # confusion matrix
mean(log_class == test_Auto$mpg01) # test error

# Using only the least noisy predictors identified in (c)
log_model <- glm(mpg01 ~ displacement + horsepower + origin + 
                   weight + cylinders, data = Auto_exc_name, family = binomial, subset = rand_index) 
log_probs <- predict(log_model, test_Auto , type = "response") # predictions for test set (not training set)
log_class <- as.factor(case_when(log_probs > 0.5 ~ "1", 
                             log_probs <= 0.5 ~ "0")) # converting probabilities to class predictions
kable(log_conf <- table(log_class, test_Auto$mpg01)) # confusion matrix
mean(log_class == test_Auto$mpg01) # test error
  
```

The best logistic model has a test accuracy of 91.1%.

#### (g) Perform KNN on the training data, with several values of K, in order to predict `mpg01`. Use only the variables that seemed most associated with `mpg01` in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?

```{r 11_g}

standardized_Auto <- scale(select(Auto, -c(name, mpg01, mpg))) # should really do this using only the test set max, min etc

glimpse(standardized_Auto)

# Set random seed, for reproducibility.
set.seed(1)
#create a list of random number ranging from 1 to number of rows from actual data and 80% 
# of the data into training data  
rand_index <- sort(sample(nrow(standardized_Auto), nrow(standardized_Auto)*0.8))
# creating training data set by selecting the output row values
train_X <- standardized_Auto[rand_index, ]
#creating test data set by not selecting the output row values
test_X <- standardized_Auto[-rand_index, ]

train_mpg01 <- Auto[rand_index, ]$mpg01 # class labels for training data
test_mpg01 <- Auto[-rand_index, ]$mpg01 # class labels for training data

knn_Auto <- function(k_num){
  set.seed(100)
  knn_pred <- knn(train_X, test_X, train_mpg01, k = k_num)
  acc <- mean(knn_pred == test_mpg01) # test error
  return(acc)
}

test_error <- NULL
counter <- 0
for (k_num in seq(1, by = 2, len = 10)) {
  counter <- counter + 1
  test_error[counter] <- knn_Auto(k_num)
  #test_error[k_num, ] <- acc
  #acc
}

k_num <- seq(1, by = 2, len = 10) # using a sequence of odd numbers to avoid the need for tie-breaks
(data_acc <- data.frame(cbind(k_num, test_error)))

ggplot(data = data_acc, aes(x = k_num, y = test_error)) +
  geom_point()

```

Best test accuracy is 94.9% for K = 13 (and 15 and 19).

## Exercise 12

**This problem involves writing functions.**

#### (a) Write a function, Power(), that prints out the result of raising 2 to the 3rd power. In other words, your function should compute 2^3 and print out the results. Hint: Recall that x^a raises x to the power a. Use the `print()` function to output the result.

```{r 12_a}
Power <- function(){
  print (2^3)
}

Power()
```

#### (b) Create a new function, `Power2()`, that allows you to pass any two numbers, x and a, and prints out the value of x^a. You can do this by beginning your function with the line `Power2 = function(x,a){` You should be able to call your function by entering, for instance, `Power2(3,8)` on the command line. This should output the value of 3^8.

```{r 12_b}
Power2 <- function(x, a){
  print (x^a)
}

Power2(3,8)
```

#### (c) Using the `Power2()` function that you just wrote, compute 10^3, 8^17, and 131^3.

```{r 12_c}
Power2(10,3) #10^3
Power2(8,17) #8^17
Power2(131,3) #131^3
```

#### (d) Now create a new function, `Power3()`, that actually returns the result x^a as an R object, rather than simply printing it to the screen. That is, if you store the value x^a in an object called `result` within your function, then you can simply `return()` this result, using the following line: `return(result)` The line above should be the last line in your function, before the `}` symbol.

```{r 12_d}
Power3 <- function(x, a){
  result <- x^a
  #return(result)
  result # return() is not necessary
}

Power3(10,3)

```


#### (e) Now using the `Power3()` function, create a plot of $f(x) = x^2$. The x-axis should display a range of integers from 1 to 10, and the y-axis should display $x^2$. Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the x-axis, the y-axis, or both on the log-scale. You can do this by using `log="x"`, `log="y"`, or `log="xy"` as arguments to the `plot()` function.

```{r 12_e}

index <- 1:10
plot(index, Power3(index, 2), main="Using Power3()",
        xlab = "x",
        ylab = "f(x) = x^2")

plot(index, Power3(index, 2), log = "x", main="Using Power3(), log x-axis",
        xlab = "x",
        ylab = "f(x) = x^2")

plot(index, Power3(index, 2), log = "y", main="Using Power3(), log y-axis",
        xlab = "x",
        ylab = "f(x) = x^2")

plot(index, Power3(index, 2), log = "xy", main="Using Power3(), log x and y-axis",
        xlab = "x",
        ylab = "f(x) = x^2")

```

#### (f) Create a function, `PlotPower()`, that allows you to create a plot of `x` against `x^a` for a fixed `a` and for a range of values of `x`. For instance, if you call `PlotPower(1:10, 3)` then a plot should be created with an x-axis taking on values $1, 2, . . . , 10$, and a y-axis taking on values $1^3, 2^3, . . . , 10^3$.

```{r 12_f}
PlotPower <- function(x, a){
  plot(x, x^a, main="Using PlotPower()",
        xlab = "x",
        ylab = "f(x) = x^a")
}

PlotPower(1:10, 3)
```

## Exercise 13

#### Using the `Boston` data set, fit classification models in order to predict whether a given suburb has a crime rate above or below the median. Explore logistic regression, LDA, and KNN models using various subsets of the predictors. Describe your findings.

```{r 13_EDA, fig.height=10}
glimpse(Boston)

# Creating a binary variable, `crim01`, that contains a 1 if `crim` contains a 
# value above its median, and a 0 if `crim` contains a value below its median
Boston_crim01 <- mutate(Boston, crim01 = as.factor(case_when(crim > median(crim) ~ 1, 
                             crim <= median(crim) ~ 0))) 

# getting data in the right format to use for facet_wrap. Excluding name and origin:
Boston_wrap <- gather(Boston_crim01, -c("crim01"), key = "vars", value = "values") 

# Using combined violin and boxplots to visualize possible relationships:
ggplot(Boston_wrap, aes(y = values, x = crim01)) +
    geom_violin() +
    geom_boxplot(width=0.1) +
    facet_wrap(~ vars, scales = "free_y", ncol = 2)

```
