---
title: "Ch4 Classification Applied Exercises"
author: "Kylie"
date: "17/04/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("ISLR")
library(MASS) # for lda, and contains a large collection of datasets
library(ISLR) # data sets for the ISLR book
library(GGally) # for ggpairs
library(tidyverse) # loads packages including ggplot2, dplyr
#library(skimr) # for nicer summaries using skim
#library(cowplot) # for plot_grid
#library(car) # for vif()
#library(olsrr) # for studentized residual plots
library(knitr) # for kable
#library(ggplot2)
library (class) # for knn()
```

## Exercise 10

This question should be answered using the `Weekly` data set, which
is part of the `ISLR` package. This data is similar in nature to the
`Smarket` data from this chapterâ€™s lab, except that it contains 1,089
weekly returns for 21 years, from the beginning of 1990 to the end of
2010.

**(a) Produce some numerical and graphical summaries of the `Weekly` data. Do there appear to be any patterns?**

```{r 10_a, fig.height = 19, fig.width = 15}
# taking an initial look at the data
names(Weekly)

as_tibble(Weekly)

summary(Weekly)

ggpairs(Weekly)

#cor(select(Weekly, Volume, Year))

# plot of all variables versus the predictor (direction)
theme_set(theme_bw()) # setting theme to black and white
Weekly_wrap <- gather(Weekly, -Direction, key = "vars", value = "values") # getting data in the right format to use for facet_wrap. 

ggplot(Weekly_wrap, aes(y = values, x = Direction)) +
    geom_boxplot() +
    facet_wrap(~ vars, scales = "free_y", ncol = 2)

```

The only strong correlation is between `Volume` and `Year` (0.84); all other correlations have an absolute value less than 0.1.

There are more days for which the market goes up than those for which it goes down.

The only clear relationship between `Direction` and the other variables is between `Direction` and `Today`, but this is just due to the definition of `Direction`

**(b) Use the full data set to perform a logistic regression with `Direction` as the response and the five lag variables plus `Volume` as predictors. Use the `summary` function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?**

```{r 10_b}
log_model <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)

summary(log_model)

```

Only `Lag2` is statistically significant (p-value of `r round(coef(summary(log_model))[3,4], 2)`).

**(c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.**

```{r 10_c}
contrasts(Weekly$Direction)

log_probs <- predict(log_model, Weekly, type = "response") # predicting probability of Up for entire dataset
glm_pred <- as.factor(case_when(log_probs > 0.5 ~ "Up", 
                             log_probs <= 0.5 ~ "Down")) # converting probabilities to class predictions

(log_conf <- table(glm_pred, Weekly$Direction)) # confusion matrix

mean(glm_pred == Weekly$Direction) # accuracy
log_conf[2,1]/(log_conf[1,1] + log_conf[2,1]) # false positive rate (fraction of negative examples that are classified as positive )
log_conf[1,2]/(log_conf[1,2] + log_conf[2,2]) # false negative rate (fraction of positive examples that are classified as negative)

#rep("Yes",length(Weekly$Direction))
```

If we naively assumed `Up` for all observations we would have an accuracy of `r round(100*mean("Up" == Weekly$Direction),2)`%, which is very similar to the accuracy of the logistic regression on the training set (so our logistic regression is not performing well even for the training set).

The false negative rate is low (7.9%), but the false positive rate is very high (89%), so logistic regression is incorrectly classifying a lot of `Down` observations as `Up`.

**(d) Now fit the logistic regression model using a training data period from 1990 to 2008, with `Lag2` as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).**

```{r 10_d}
train <- (Weekly$Year < 2009) # a Boolean vector (elements are TRUE or FALSE) the same length as Year. The elements of the vector that correspond to observations that occurred before 2009 are set to TRUE, whereas those that correspond to observations after 2009 are set to FALSE

log_model <- glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train) # fitting logistic with Lag2 as the only predictor, using just the data from 1990 to 2008
log_probs <- predict(log_model, Weekly[!train, ], type = "response") # predicting probability of Up for test set (not training set)
glm_pred <- as.factor(case_when(log_probs > 0.5 ~ "Up", 
                             log_probs <= 0.5 ~ "Down")) # converting probabilities to class predictions
(log_conf <- table(glm_pred, Weekly$Direction[!train])) # confusion matrix
mean(glm_pred == Weekly$Direction[!train]) # accuracy
log_conf[2,1]/(log_conf[1,1] + log_conf[2,1]) # false positive rate (fraction of negative examples that are classified as positive )
log_conf[1,2]/(log_conf[1,2] + log_conf[2,2]) # false negative rate (fraction of positive examples that are classified as negative)
```

The accuracy is improved.

**(e) Repeat (d) using LDA.**

```{r 10_e}
lda_model <- lda(Direction ~ Lag2, data = Weekly, subset = train) # fitting lda with Lag2 as the only predictor, using just the data from 1990 to 2008
lda_class <- predict(lda_model, Weekly[!train, ])$class # predictions for test set (not training set)
(lda_conf <- table(lda_class, Weekly$Direction[!train])) # confusion matrix
mean(lda_class == Weekly$Direction[!train]) # accuracy
lda_conf[2,1]/(lda_conf[1,1] + lda_conf[2,1]) # false positive rate (fraction of negative examples that are classified as positive )
lda_conf[1,2]/(lda_conf[1,2] + lda_conf[2,2]) # false negative rate (fraction of positive examples that are classified as negative)
```

Accuracy of LDA is the same as the accuracy of logistic regression model.

**(f) Repeat (d) using QDA.**

```{r 10_f}
qda_model <- qda(Direction ~ Lag2, data = Weekly, subset = train) # fitting qda with Lag2 as the only predictor, using just the data from 1990 to 2008
qda_class <- predict(qda_model, Weekly[!train, ])$class # predictions for test set (not training set)
(qda_conf <- table(qda_class, Weekly$Direction[!train])) # confusion matrix
mean(qda_class == Weekly$Direction[!train]) # accuracy
qda_conf[2,1]/(qda_conf[1,1] + qda_conf[2,1]) # false positive rate (fraction of negative examples that are classified as positive )
qda_conf[1,2]/(qda_conf[1,2] + qda_conf[2,2]) # false negative rate (fraction of positive examples that are classified as negative)
```

Accuracy is lower compared to LDA and logistic regression.

**(g) Repeat (d) using KNN with K = 1.**

```{r 10_g}
train_X <- data.frame(Weekly$Lag2[train]) # training set predictors. data.frame is required because knn interprets a vector as a single observation of multiple predictors (with the number of predictors equal to the length of the vector)
test_X <- data.frame(Weekly$Lag2[!train]) # test set predictors
train_Direction <- Weekly$Direction[train] # class labels for training data

set.seed(1) # for reproducibility because if several observations are tied as nearest neighbors then R will randomly break the tie.
knn_pred <- knn(train_X, test_X, train_Direction, k = 1)
(knn_conf <- table(knn_pred, Weekly$Direction[!train])) # confusion matrix
mean(knn_pred == Weekly$Direction[!train]) # accuracy
knn_conf[2,1]/(knn_conf[1,1] + knn_conf[2,1]) # false positive rate (fraction of negative examples that are classified as positive )
knn_conf[1,2]/(knn_conf[1,2] + knn_conf[2,2]) # false negative rate (fraction of positive examples that are classified as negative)
```

KNN has the worst overall accuracy.

**(h) Which of these methods appears to provide the best results on this data?**

Linear regression and LDA give the best overall accuracy.

**(i) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.**

## Exercise 11

**In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the `Auto` data set.**

**(a) Create a binary variable, `mpg01`, that contains a 1 if `mpg` contains a value above its median, and a 0 if `mpg` contains a value below its median. You can compute the median using the `median()` function. Note you may find it helpful to use the `data.frame()` function to create a single data set containing both `mpg01` and the other `Auto` variables.**

```{r 11_a}

Auto <- mutate(Auto, mpg01 = as.factor(case_when(mpg > median(mpg) ~ 1, 
                             mpg <= median(mpg) ~ 0))) # Creating a binary variable, `mpg01`, that contains a 1 if `mpg` contains a value above its median, and a 0 if `mpg` contains a value below its median
```


**(b) Explore the data graphically in order to investigate the association between `mpg01` and the other features. Which of the other features seem most likely to be useful in predicting `mpg01`? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.**

```{r 11_b}

Auto_wrap <- gather(Auto, -c("mpg01", "name", "origin"), key = "vars", value = "values") # getting data in the right format to use for facet_wrap. Excluding name and origin

ggplot(Auto_wrap, aes(y = values, x = mpg01)) +
    geom_boxplot() +
    facet_wrap(~ vars, scales = "free_y", ncol = 2)

ggplot(Auto_wrap, aes(y = values, x = mpg01)) +
    geom_violin() +
    geom_boxplot(width=0.1) +
    facet_wrap(~ vars, scales = "free_y", ncol = 2)

```

```{r 11_b_factor}

count_Auto <- count(Auto, origin, mpg01)
kable(spread(count_Auto, mpg01, n, fill = 0))

ggplot(data = Auto) +
  geom_count(mapping = aes(x = origin, y = mpg01))

ggplot(data = Auto) +
  geom_count(mapping = aes(x = as.factor(cylinders), y = mpg01))

count_Auto_cylinders <- count(Auto, cylinders, mpg01)
kable(spread(count_Auto_cylinders, mpg01, n, fill = 0))

```

All variables (except `name`) look useful for predicting `mgp01`.

**(c) Split the data into a training set and a test set.**

```{r 11_c}

# Set random seed, for reproducibility.
set.seed(1)

#create a list of random number ranging from 1 to number of rows from actual data and 80% of the data into training data  
rand_index <- sort(sample(nrow(Auto), nrow(Auto)*0.8))

# creating training data set by selecting the output row values
train_Auto <- Auto[rand_index, ]

#creating test data set by not selecting the output row values
test_Auto <- Auto[-rand_index, ]

```

**(d) Perform LDA on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?**

```{r 11_d}
# Linear Discrimminant Analysis
# All variables as predictors except name
lda_model <- lda(mpg01 ~.-name, data = Auto, subset = rand_index) # fitting lda with all predictors except name using training day
lda_class <- predict(lda_model, test_Auto)$class # predictions for test set (not training set)
(lda_conf <- table(lda_class, test_Auto$mpg01)) # confusion matrix
mean(lda_class == test_Auto$mpg01) # test error

# Using only the least noisy predictors identified in (c)
lda_model <- lda(mpg01 ~ displacement + horsepower + origin + weight + cylinders, data = Auto, subset = rand_index) # fitting lda with all predictors except name using training day
lda_class <- predict(lda_model, test_Auto)$class # predictions for test set (not training set)
(lda_conf <- table(lda_class, test_Auto$mpg01)) # confusion matrix
mean(lda_class == test_Auto$mpg01) # test error

```

**(e) Perform QDA on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?**

```{r 11_e}
# Quadratic Discrimminant Analysis
# All variables as predictors except name
qda_model <- qda(mpg01 ~.-name, data = Auto, subset = rand_index) # fitting qda with all predictors except name using training day
qda_class <- predict(qda_model, test_Auto)$class # predictions for test set (not training set)
(qda_conf <- table(qda_class, test_Auto$mpg01)) # confusion matrix
mean(qda_class == test_Auto$mpg01) # test error

# Using only the least noisy predictors identified in (c)
qda_model <- qda(mpg01 ~ displacement + horsepower + origin + weight + cylinders, data = Auto, subset = rand_index) # fitting qda with all predictors except name using training day
qda_class <- predict(qda_model, test_Auto)$class # predictions for test set (not training set)
(qda_conf <- table(qda_class, test_Auto$mpg01)) # confusion matrix
mean(qda_class == test_Auto$mpg01) # test error
```

**(f) Perform logistic regression on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?**

```{r 11_f}
# Logistic regression
# All variables as predictors except name
# log_model <- glm(mpg01 ~.-name, data = Auto, family = binomial, subset = rand_index) # fitting qda with all predictors except name using training day
# log_probs <- predict(log_model, test_Auto, type = "response")$class # predictions for test set (not training set)
# log_class <- as.factor(case_when(log_probs > 0.5 ~ "1", 
#                              log_probs <= 0.5 ~ "0")) # converting probabilities to class predictions
# (log_conf <- table(log_class, test_Auto$mpg01)) # confusion matrix
# mean(log_class == test_Auto$mpg01) # test error
# 
# # Using only the least noisy predictors identified in (c)
# log_model <- glm(mpg01 ~ displacement + horsepower + origin + weight + cylinders, data = Auto, family = binomial, subset = rand_index) # fitting qda with all predictors except name using training day
# log_probs <- predict(log_model, test_Auto, type = "response")$class # predictions for test set (not training set)
# log_class <- as.factor(case_when(log_probs > 0.5 ~ "1", 
#                              log_probs <= 0.5 ~ "0")) # converting probabilities to class predictions
# (log_conf <- table(log_class, test_Auto$mpg01)) # confusion matrix
# mean(log_class == test_Auto$mpg01) # test error
```

**(g) Perform KNN on the training data, with several values of K, in order to predict `mpg01`. Use only the variables that seemed most associated with `mpg01` in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?**

```{r 11_g}
standardized_Auto <- scale(select(Auto, -c(name, mpg01))) # should really do this using only the test set max, min etc

# Set random seed, for reproducibility.
set.seed(1)
#create a list of random number ranging from 1 to number of rows from actual data and 80% of the data into training data  
rand_index <- sort(sample(nrow(standardized_Auto), nrow(standardized_Auto)*0.8))
# creating training data set by selecting the output row values
train_X <- standardized_Auto[rand_index, ]
#creating test data set by not selecting the output row values
test_X <- standardized_Auto[-rand_index, ]

train_mpg01 <- Auto[rand_index, ]$mpg01 # class labels for training data
test_mpg01 <- Auto[-rand_index, ]$mpg01 # class labels for training data
 

# k = 1
knn_pred <- knn(train_X, test_X, train_mpg01, k = 1)
(knn_conf <- table(knn_pred, test_mpg01)) # confusion matrix
acc_1 <- mean(knn_pred == test_mpg01) # test error

# k = 2
knn_pred <- knn(train_X, test_X, train_mpg01, k = 2)
(knn_conf <- table(knn_pred, test_mpg01)) # confusion matrix
acc_2 <- mean(knn_pred == test_mpg01) # test error

# k = 3
knn_pred <- knn(train_X, test_X, train_mpg01, k = 3)
(knn_conf <- table(knn_pred, test_mpg01)) # confusion matrix
acc_3 <- mean(knn_pred == test_mpg01) # test error

# k = 5
knn_pred <- knn(train_X, test_X, train_mpg01, k = 5)
(knn_conf <- table(knn_pred, test_mpg01)) # confusion matrix
acc_5 <- mean(knn_pred == test_mpg01) # test error

# k = 10
knn_pred <- knn(train_X, test_X, train_mpg01, k = 10)
(knn_conf <- table(knn_pred, test_mpg01)) # confusion matrix
acc_10 <- mean(knn_pred == test_mpg01) # test error

K <- c(1, 2, 3, 5, 10)
acc <- c(acc_1, acc_2, acc_3, acc_5, acc_10)
data_acc <- as_tibble(K, acc)

ggplot(data = data_acc, aes(x = K, y = acc)) +
  geom_point()

knn_Auto <- function(k_num){
  knn_pred <- knn(train_X, test_X, train_mpg01, k = k_num)
  acc <- mean(knn_pred == test_mpg01) # test error
  return(acc)
}

test_error <- matrix(nrow = 20, ncol = 1)
for (k_num in 1:20) {
  knn_Auto(k_num)
  #test_error[k_num, ] <- acc
  #acc
}

k_num <- 1:20
(data_acc <- as_tibble(k_num, test_error))

ggplot(data = data_acc, aes(x = k_num, y = test_error)) +
  geom_point()

```


## Exercise 12

**This problem involves writing functions.**

**(a) Write a function, Power(), that prints out the result of raising 2 to the 3rd power. In other words, your function should compute 2^3 and print out the results. Hint: Recall that x^a raises x to the power a. Use the `print()` function to output the result.**

```{r 12_a}
Power <- function(){
  print (2^3)
}

Power()
```

**(b) Create a new function, Power2(), that allows you to pass any two numbers, x and a, and prints out the value of x^a. You can do this by beginning your function with the line `Power2 = function(x,a){` You should be able to call your function by entering, for instance, `Power2(3,8)` on the command line. This should output the value of 3^8.**

```{r 12_b}
Power2 <- function(x, a){
  print (x^a)
}

Power2(3,8)
```

**(c) Using the `Power2()` function that you just wrote, compute 10^3, 8^17, and 131^3.**

```{r 12_c}
Power2(10,3) #10^3
Power2(8,17) #8^17
Power2(131,3) #131^3
```

**(d) Now create a new function, `Power3()`, that actually returns the result x^a as an R object, rather than simply printing it to the screen. That is, if you store the value x^a in an object called `result` within your function, then you can simply `return()` this result, using the following line: `return(result)` The line above should be the last line in your function, before the `}` symbol.**

```{r 12_d}
Power3 <- function(x, a){
  result <- x^a
  #return(result)
  result # return() is not necessary
}

Power3(10,3)
```


**(e) Now using the `Power3()` function, create a plot of $f(x) = x^2$. The x-axis should display a range of integers from 1 to 10, and the y-axis should display $x^2$. Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the x-axis, the y-axis, or both on the log-scale. You can do this by using `log="x"`, `log="y"`, or `log="xy"` as arguments to the `plot()` function.**

```{r 12_e}

index <- 1:10
plot(index, Power3(index, 2), main="Using Power3()",
        xlab = "x",
        ylab = "f(x) = x^2")

plot(index, Power3(index, 2), log = "x", main="Using Power3(), log x-axis",
        xlab = "x",
        ylab = "f(x) = x^2")

plot(index, Power3(index, 2), log = "y", main="Using Power3(), log y-axis",
        xlab = "x",
        ylab = "f(x) = x^2")

plot(index, Power3(index, 2), log = "xy", main="Using Power3(), log x and y-axis",
        xlab = "x",
        ylab = "f(x) = x^2")

```

**(f) Create a function, `PlotPower()`, that allows you to create a plot of `x` against `x^a` for a fixed `a` and for a range of values of `x`. For instance, if you call `PlotPower(1:10, 3)` then a plot should be created with an x-axis taking on values $1, 2, . . . , 10$, and a y-axis taking on values $1^3, 2^3, . . . , 10^3$.**

```{r 12_f}
PlotPower <- function(x, a){
  plot(x, x^a, main="Using PlotPower()",
        xlab = "x",
        ylab = "f(x) = x^a")
}

PlotPower(1:10, 3)
```

## Exercise 13

**Using the `Boston` data set, fit classification models in order to predict whether a given suburb has a crime rate above or below the median. Explore logistic regression, LDA, and KNN models using various subsets of the predictors. Describe your findings.**