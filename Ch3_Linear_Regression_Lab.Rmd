---
title: 'Ch3: Linear Regression' 
subtitle: 'Lab'
author: "Kylie"
date: "06/04/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("ISLR")
library(MASS) # large collection of datasets
library(ISLR) # data sets for the ISLR book
#library(GGally) # for ggpairs
library(tidyverse) # loads packages including ggplot2, dplyr
#library(skimr) # for nicer summaries using skim
#library(cowplot) # for plot_grid
library(car) # for vif()
```

# 3.6.2 Simple Linear Regression

Using the `lm()` function to fit a simple linear regression model. The basic
syntax is `lm(y ∼ x, data)`, where `y` is the response, `x` is the predictor, and
`data` is the data set in which these two variables are kept.

```{r simple_regression}
# Looking at the data briefly:
#fix(Boston) # or View(Boston)
names(Boston) # lists the variable names

as_tibble(Boston) # alternative way to view a subsection of the data

# using the lm() function to fit a simple linear regression lm()
# model, with medv as the response and lstat as the predictor:
lm_fit <- lm(medv∼lstat, data=Boston)

lm_fit # outputs some basic information about the model

summary(lm_fit) # outputs more detail including pvalues and standard errors for the coefficients, as well as the R2 statistic and F-statistic for the model.

names(lm_fit) # shows what other pieces of information are stored in lm_fit.

# Although we can extract these quantities by name—e.g. lm.fit$coefficients—it is safer to use the extractor functions like coef() to access them.
coef(lm_fit)

confint(lm_fit) # confidence interval for the coefficient estimates

predict(lm_fit, data.frame(lstat = c(5, 10, 15)), interval = "confidence") # produces confidence intervals for the prediction of medv for a given value of lstat

predict(lm_fit, data.frame(lstat = c(5, 10, 15)), interval = "prediction") # produces prediction intervals for the prediction of medv for a given value of lstat

# plotting data and fitted line
attach(Boston)
plot(lstat, medv)
abline (lm_fit)
# trying different linewidths, symbol shapes and colours
abline(lm_fit, lwd =3)
abline(lm_fit, lwd =3, col ="red")
plot(lstat, medv, col = "red")
plot(lstat, medv, pch =20)
plot(lstat, medv, pch = "+")
plot (1:20, 1:20, pch =1:20)

# ggplot alternative:
theme_set(theme_bw()) # setting theme to black and white

ggplot(Boston, aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
# trying different linewidths, symbol shapes and colours
ggplot(Boston, aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, lwd = 3, color = "red")

ggplot(Boston, aes(x = lstat, y = medv)) +
  geom_point(color = "red", shape = 21, size = 3)

ggplot(Boston, aes(x = lstat, y = medv)) +
  geom_point(shape = "+", size = 3)

ggplot() +
  geom_point(aes(x = 1:20, y = 1:20), shape = 1:20, size = 3)
```

Need to look up the difference between confidence intervals and prediction intervals.

Four diagnostic plots are automatically produced by applying the `plot()` function directly to the output from `lm()`. Detailed information about these plots is available at: https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html

```{r simple_regression_diag}
par(mfrow = c(2,2)) # divides the plotting region into a 2 × 2 grid of panels
plot(lm_fit) # plotting diagnostic plots

plot(predict(lm_fit), residuals(lm_fit))
plot(predict(lm_fit), rstudent(lm_fit))
# On the basis of the residual plots, there is some evidence of non-linearity.

plot(hatvalues(lm_fit)) # computes leverage statistics for any number of predictors
which.max(hatvalues(lm_fit)) # tells us which observation has the largest leverage statistic
```

# 3.6.3 Multiple Linear Regression

Using the `lm()` function to fit a multiple linear regression model. The
syntax is `lm(y ∼ x1 + x2 + x3, data)`, where `y` is the response, `x1, x2, x3` are the predictors, and
`data` is the data set in which these two variables are kept.

```{r mult_regression}

lm_fit <- lm(medv ∼ lstat + age, data = Boston) # fitting multiple linear regression model with lstat and age as predictors and medv as the response
summary(lm_fit)

lm_fit_all <- lm(medv ∼., data = Boston) # fitting multiple linear regression model with ALL predictors and medv as the response
summary(lm_fit_all)

summary(lm_fit)$r.sq # gives the R2

summary(lm_fit)$sigma # gives the RSE (Residual Standard Error: Roughly speaking, RSE is the average amount that the response will deviate from the true regression line).

vif(lm_fit) # gives variance inflation factors

lm_fit1 <- lm(medv ∼. -age, data = Boston) # fitting all variables EXCEPT age
summary(lm_fit1)
```

# 3.6.4 Interaction Terms

```{r interaction}
summary(lm(medv ∼ lstat*age, data = Boston)) # includes lstat, age, and the interaction term lstat×age as predictors
```

# 3.6.5 Non-linear Transformations of the Predictors

The `lm()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor `X`, we can create a predictor `X^2` using `I(X^2)`. The function `I()` is needed since the `^` has a special meaning in a formula; wrapping as we do allows the standard usage in R, which is to raise `X` to the power 2.

```{r nonlinear}
lm_fit2 <- lm(medv ∼ lstat + I(lstat^2)) # regression of medv onto lstat and lstat^2
summary(lm_fit2) 
# The near-zero p-value associated with the quadratic term suggests that it leads to an improved model.

lm_fit <- lm(medv ∼ lstat) 
anova(lm_fit, lm_fit2) # to quantify the extent to which the quadratic fit is superior to the linear fit.
```

"Here Model 1 represents the linear submodel containing only one predictor,
`lstat`, while Model 2 corresponds to the larger quadratic model that has two
predictors, `lstat` and `lstat^2`. The `anova()` function performs a hypothesis
test comparing the two models. The null hypothesis is that the two models
fit the data equally well, and the alternative hypothesis is that the full
model is superior. Here the F-statistic is 135 and the associated p-value is
virtually zero. This provides very clear evidence that the model containing
the predictors `lstat` and `lstat^2` is far superior to the model that only
contains the predictor `lstat`. This is not surprising, since earlier we saw
evidence for non-linearity in the relationship between `medv` and `lstat`."

```{r polynomial}
par(mfrow = c(2,2))
plot(lm_fit2)
# there is little discernible pattern in the residuals.

lm_fit5 <- lm(medv ∼ poly(lstat, 5)) # a fifth-order polynomial fit
summary(lm_fit5)

summary(lm(medv ∼ log(rm), data = Boston)) # log transformation of rm
```

# 3.6.6 Qualitative Predictors

Given a qualitative variable such as `Shelveloc` (which takes on 3 possible values, Bad, Medium, and Good), `R` generates dummy variables automatically.

BUT ordering is important for `Shelveloc`, does it make a difference if we transform it to a factor with values 1, 2 and 3, or is ordering not important for linear regression?

```{r qualitative}
as_tibble(Carseats)
names(Carseats)

lm_fit <- lm(Sales ∼. + Income:Advertising + Price:Age, data = Carseats) # multiple regression model that includes all predictors plus some interaction terms
summary(lm_fit)

#contrasts(Carseats$ShelveLoc)
```


# New packages/functions 

New packages/functions I've learnt during these exercises:

- `lm()`

`lm(y ∼ x, data = data)`: fits `x`

`lm(y ∼ x1 + x2, data = data)`: fits `x2` and `x1`

`lm(y ∼., data = data)`: fits all variables in `data`

`lm(y ∼. -x1, data = data)`: fits all variables in `data` except `x1`

`lm(y ~ x1:x2, data = data)`: includes the interaction term `x1` x `x2` as a predictor.

`lm(y ~ x1*x2, data = data)`: includes `x1`, `x2` and the interaction term `x1` x `x2`; it is a shorthand for
`lstat` + `age` + `lstat:age` 

`lm(y ~ poly(x, 5), data = data)`: fits `x`, `x^2`, `x^3`, `x^4` and `x^5`.

- `predict()`

- `geom_smooth()`

- `residuals()`

- `rstudent()`

- `which.max()`: identifies the index of the largest element of a vector.

- `hatvalues()`: need to find out what this does.

- `I()`

- `contrasts()`

## `confit`

Computes confidence intervals for one or more parameters in a fitted model. 

`confint(object, parm, level = 0.95, …)`

Arguments/Inputs:

- object: a fitted model object.

- parm: a specification of which parameters are to be given confidence intervals, either a vector of numbers or a vector of names. If missing, all parameters are considered.

- level: the confidence level required (0.95 is the default).

Value/Output:

A matrix (or vector) with columns giving lower and upper confidence limits for each parameter.