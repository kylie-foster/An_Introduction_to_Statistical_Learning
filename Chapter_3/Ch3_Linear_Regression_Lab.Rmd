---
title: 'Ch3: Linear Regression' 
subtitle: 'Lab'
author: "Kylie"
date: "06/04/2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("ISLR")
library(MASS) # large collection of datasets
library(ISLR) # data sets for the ISLR book
#library(GGally) # for ggpairs
#library(tidyverse) # loads packages including ggplot2, dplyr
#library(skimr) # for nicer summaries using skim
#library(cowplot) # for plot_grid
library(car) # for vif()
library(olsrr) # for studentized residual plots
```

# 3.6.2 Simple Linear Regression

Using the `lm()` function to fit a simple linear regression model. The basic
syntax is `lm(y ∼ x, data)`, where `y` is the response, `x` is the predictor, and
`data` is the data set in which these two variables are kept.

```{r simple_regression}
# Looking at the data briefly:
#fix(Boston) # or View(Boston)
names(Boston) # lists the variable names

as_tibble(Boston) # alternative way to view a subsection of the data

# using the lm() function to fit a simple linear regression lm()
# model, with medv as the response and lstat as the predictor:
lm_fit <- lm(medv∼lstat, data=Boston)

lm_fit # outputs some basic information about the model

summary(lm_fit) # outputs more detail including pvalues and standard errors for the coefficients, as well as the R2 statistic and F-statistic for the model.

names(lm_fit) # shows what other pieces of information are stored in lm_fit.

# Although we can extract these quantities by name—e.g. lm.fit$coefficients—it is safer to use the extractor functions like coef() to access them.
coef(lm_fit)

confint(lm_fit) # confidence interval for the coefficient estimates

predict(lm_fit, data.frame(lstat = c(5, 10, 15)), interval = "confidence") # produces confidence intervals for the prediction of medv for a given value of lstat

predict(lm_fit, data.frame(lstat = c(5, 10, 15)), interval = "prediction") # produces prediction intervals for the prediction of medv for a given value of lstat
```

The difference between confidence intervals and prediction intervals:

Even if we knew f(X) (i.e., even if we knew the true values
for $\beta_0, \beta_1, . . . , \beta_p$) the response value cannot be predicted perfectly
because of the random (irreducible) error $\epsilon$ in the model ($Y = f(X) + \epsilon$). How much will $Y$ vary from
$\widehat{Y}$? We use **prediction intervals** to answer this question. Prediction
intervals are always wider than confidence intervals, because they
incorporate both the error in the estimate for $f(X)$ (the reducible
error) and the uncertainty as to how much an individual point will
differ from the population regression plane (the irreducible error).

```{r simple_regression_plots}
# plotting data and fitted line
attach(Boston)
plot(lstat, medv)
abline (lm_fit)
# trying different linewidths, symbol shapes and colours
abline(lm_fit, lwd =3)
abline(lm_fit, lwd =3, col ="red")
plot(lstat, medv, col = "red")
plot(lstat, medv, pch =20)
plot(lstat, medv, pch = "+")
plot (1:20, 1:20, pch =1:20)

# ggplot alternative:
theme_set(theme_bw()) # setting theme to black and white

ggplot(Boston, aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
# trying different linewidths, symbol shapes and colours
ggplot(Boston, aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, lwd = 3, color = "red")

ggplot(Boston, aes(x = lstat, y = medv)) +
  geom_point(color = "red", shape = 21, size = 3)

ggplot(Boston, aes(x = lstat, y = medv)) +
  geom_point(shape = "+", size = 3)

ggplot() +
  geom_point(aes(x = 1:20, y = 1:20), shape = 1:20, size = 3)
```

Four diagnostic plots are automatically produced by applying the `plot()` function directly to the output from `lm()`. Detailed information about these plots is available at: https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html

```{r simple_regression_diag}
par(mfrow = c(2,2)) # divides the plotting region into a 2 × 2 grid of panels
plot(lm_fit) # plotting diagnostic plots
```

```{r simple_regression_diag_manual}
plot(predict(lm_fit), residuals(lm_fit)) # compute the residuals from a linear regression fit using residuals()
plot(predict(lm_fit), rstudent(lm_fit)) # rstudent() will return the studentized residuals,
# On the basis of the residual plots, there is some evidence of non-linearity.

plot(hatvalues(lm_fit)) # computes leverage statistics for any number of predictors
which.max(hatvalues(lm_fit)) # tells us which observation has the largest leverage statistic
```

## Looking at the diagnostic plots in more detail.

### Residual plot

```{r diag_1}
plot(lm_fit, which = 1)
```

The first plot is a **residual plot** is a plot of the residuals (actual y - predicted y) versus the predicted values of y. Ideally, the residual plot will show **no discernible pattern**. The red line is a smooth fit to the residuals that is displayed
in order to make it easier to identify any trends. The residual plot is used to check:

 - **that there is a straight-line relationship** between the predictors and the response. The plot above shows a
U-shape, which provides an indication of non-linearity in the data. Other things to look for: any trend in the residuals; “funneling” phenomenon (when the distribution of the residuals is quite well concentrated around 0 for small fitted values, but they get more and more spread out as the fitted values increase, this indicates “increasing variance”).

If the true relationship is far from linear, then virtually all of the conclusions that we draw from the
fit are suspect and the prediction accuracy of the model can be significantly reduced.

What to do if a non-linear relationship is indicated by the residual plot? Use non-linear transformations of the
predictors, such as logX, √X, and X^2, in the regression model. For example, fitting a quadratic removes the trend shown in the residuals plot (see below). Alternatively, use more advanced non-linear alternatives to linear regression.

```{r diag_1_quad}
lm_fit_quad <- lm(medv ∼ lstat + I(lstat^2), data=Boston)
plot(lm_fit_quad, which = 1)
```

- **correlation of error terms**. An important assumption of the linear regression model is that the error
terms are uncorrelated. If the errors are uncorrelated, then the fact that $\epsilon_i$ is positive provides
little or no information about the sign of $\epsilon_{i+1}$. If the errors are uncorrelated, then there should be no discernible pattern. On the other hand, if the error terms are positively correlated, then
adjacent residuals may have similar values (tracking in the residuals). Correlation of error terms frequently occurs in the context of time series data. Why does this matter? The standard errors that
are calculated for the estimated regression coefficients or the fitted y values are based on the assumption of uncorrelated error terms. If there is correlation among the error terms, then the estimated standard errors
will tend to underestimate the true standard errors and as a result confidence
and prediction intervals will be narrower than they should be (the model will be overconfident). 

- **non-constant variance of error terms** (or heteroscedasticity). Another important assumption of the linear regression model is that the error terms have a constant variance, $Var(\epsilon_i) = \sigma^2$. The standard errors,
confidence intervals, and hypothesis tests associated with the linear model
rely upon this assumption. Unfortunately, it is often the case that the variances of the error terms are
non-constant (e.g., the variances of the error terms may increase
with the value of the response, see below). You can identify non-constant variances in
the errors from the presence of a **funnel shape** in the
residual plot.One possible solution to this problem is to transform
the response Y using a concave function such as $logY$ or $\sqrt{Y}$. These transformation result in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity.

```{r diag_1_incr}
n <- 1000      # sample size
x <- runif(n, min = 0, max = 100)
y.increasing <- 3 + 0.2 * x + (1 + x / 25) * rnorm(n, sd = 3)
lm.increasing <- lm(y.increasing ~ x)
plot(lm.increasing, which = 1)
```

- **identify outliers**. An **outlier** is a point for which **$y_i$ is far from the value predicted by the model**
In practice, it can be difficult to decide how large a residual
needs to be before we consider the point to be an outlier. To address
this problem, instead of plotting the residuals, you can plot the **studentized residuals**. Observations whose absolute value of studentized residuals are **greater than 3** are possible outliers.

It is typical for an outlier that does not have an unusual
predictor value to have little effect on the least squares fit. However, even
if an outlier does not have much effect on the least squares fit, it can cause
other problems (e.g., including or excluding an outlier can have a substantial impact on all confidence intervals, p-values and the $R^2$). If you believe that an outlier has occurred because of an error in data collection
or recording, then one solution is to remove the observation. However, care should be taken, since an outlier may instead indicate a
deficiency with the model, such as a missing predictor.

```{r diag_rstud}
# plot(rstudent(lm_fit)) # studentized residuals

plot(lm_fit$fitted.values,rstudent(lm_fit))# studentized residuals

ols_plot_resid_stud_fit(lm_fit)# externally studentized residuals
```

A perfect residual plot is displayed below.

```{r diag_1_good}
n <- 1000      # sample size
x <- runif(n, min = 0, max = 100)
y.good <- 3 + 0.1 * x + rnorm(n, sd = 3)

lm.good <- lm(y.good ~ x)

plot(lm.good, which = 1)
```

The third plot is the Scale-Location plot. The "Residuals vs Fitted" and "Scale-Location" charts are essentially the same, and show if there is a trend to the residuals.

```{r diag_3}
plot(lm_fit, which = 3)
```


### Normal QQ plot

The second plot is the **Normal QQ plot** that is used to assess whether the **residuals are normally distributed**. There can be problems if the residuals look far from normal. In particular, if the residual tend to be larger in magnitude than what we would expect from the normal distribution, then our p-values and confidence intervals may be too optimisitic.

To address this you can try transforming y (e.g., use log(y)). But it seems like non-normally distributed residuals can indicate other problems with the model (e.g., outliers could be a problem or the linear model assumption is not correct).

In our Normal QQ plot the residuals deviate from the diagonal line in both the upper and lower tail.

```{r diag_2}
plot(lm_fit, which = 2)
```

An ideal Normal QQ plot is displayed below.

```{r diag_2_good}
plot(lm.good, which = 2)
```

### Leverage

Observations with **high leverage have an unusual value for $x_i$** (as opposed to outliers, which have an unusual value for $y_i$).
It is important to identify high leverage observations because they **tend to have a sizable impact on the estimated regression line**. It is cause for concern if the least squares line is heavily affected by just a couple of observations,
because any problems with these points may invalidate the entire fit.
In a simple linear regression, high leverage observations are fairly easy to
identify, since we can simply look for observations for which the predictor
value is outside of the normal range of the observations. But in a multiple
linear regression with many predictors, it is possible to have an observation
that is well within the range of each individual predictor’s values, but that
is unusual in terms of the full set of predictors. In order to quantify an observation’s leverage, we compute the **leverage statistic**. A **large value** of this statistic **indicates an observation with high leverage**. If a given observation has a leverage statistic that greatly exceeds $(p+1)/n$ (where $p$ is the number of predictors and $n$ is the total number of observations), then we may suspect that the corresponding
point has high leverage.

```{r diag_4}
plot(lm_fit, which = 5)
```

Alternative plotting option that uses studentized residuals, rather than standardized residuals:

```{r diag_resid_lev}
ols_plot_resid_lev(lm_fit)
```

# 3.6.3 Multiple Linear Regression

Using the `lm()` function to fit a multiple linear regression model. The
syntax is `lm(y ∼ x1 + x2 + x3, data)`, where `y` is the response, `x1, x2, x3` are the predictors, and
`data` is the data set in which these two variables are kept.

```{r mult_regression}

lm_fit <- lm(medv ∼ lstat + age, data = Boston) # fitting multiple linear regression model with lstat and age as predictors and medv as the response
summary(lm_fit)

lm_fit_all <- lm(medv ∼., data = Boston) # fitting multiple linear regression model with ALL predictors and medv as the response
summary(lm_fit_all)

summary(lm_fit_all)$r.sq # gives the R2

summary(lm_fit_all)$sigma # gives the RSE (Residual Standard Error: Roughly speaking, RSE is the average amount that the response will deviate from the true regression line).

vif(lm_fit_all) # gives variance inflation factors
```

The **variance inflation factor (VIF)** is a way to assess **multicollinearity**. The VIF is
the ratio of the variance of $\hat{\beta_j}$ when fitting the full model divided by the
variance of $\hat{\beta_j}$ if fit on its own. The smallest possible value for VIF is **1**,
which indicates the **complete absence of collinearity**. As a rule of
thumb, a **VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity**.

```{r mult_regression_except}
lm_fit1 <- lm(medv ∼. -age, data = Boston) # fitting all variables EXCEPT age
summary(lm_fit1)

# alternatively using update() function:
lm_fit1 <- update(lm_fit_all, ∼. -age)
```

# 3.6.4 Interaction Terms

```{r interaction}
summary(lm(medv ∼ lstat*age, data = Boston)) # includes lstat, age, and the interaction term lstat×age as predictors
```

# 3.6.5 Non-linear Transformations of the Predictors

The `lm()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor `X`, we can create a predictor `X^2` using `I(X^2)`. The function `I()` is needed since the `^` has a special meaning in a formula; wrapping as we do allows the standard usage in R, which is to raise `X` to the power 2.

```{r nonlinear}
lm_fit2 <- lm(medv ∼ lstat + I(lstat^2)) # regression of medv onto lstat and lstat^2
summary(lm_fit2) 
# The near-zero p-value associated with the quadratic term suggests that it leads to an improved model.

lm_fit <- lm(medv ∼ lstat) 
anova(lm_fit, lm_fit2) # to quantify the extent to which the quadratic fit is superior to the linear fit.
```

"Here Model 1 represents the linear submodel containing only one predictor,
`lstat`, while Model 2 corresponds to the larger quadratic model that has two
predictors, `lstat` and `lstat^2`. The `anova()` function performs a hypothesis
test comparing the two models. The null hypothesis is that the two models
fit the data equally well, and the alternative hypothesis is that the full
model is superior. Here the F-statistic is 135 and the associated p-value is
virtually zero. This provides very clear evidence that the model containing
the predictors `lstat` and `lstat^2` is far superior to the model that only
contains the predictor `lstat`. This is not surprising, since earlier we saw
evidence for non-linearity in the relationship between `medv` and `lstat`."

```{r polynomial}
par(mfrow = c(2,2))
plot(lm_fit2)
# there is little discernible pattern in the residuals.

lm_fit5 <- lm(medv ∼ poly(lstat, 5)) # a fifth-order polynomial fit
summary(lm_fit5)

summary(lm(medv ∼ log(rm), data = Boston)) # log transformation of rm
```

# 3.6.6 Qualitative Predictors

Given a qualitative variable such as `Shelveloc` (which takes on 3 possible values, Bad, Medium, and Good), `R` generates dummy variables automatically.



```{r qualitative}
as_tibble(Carseats)
names(Carseats)

lm_fit <- lm(Sales ∼. + Income:Advertising + Price:Age, data = Carseats) # multiple regression model that includes all predictors plus some interaction terms
summary(lm_fit)

contrasts(Carseats$ShelveLoc)
```

Alternatively, converting `Shelveloc` to an ordered factor to take into account the order of the shelf locations:

```{r qualitative_ordered}
Carseats$ShelveLoc <- factor(Carseats$ShelveLoc, levels=c("Bad", "Medium", "Good"), ordered=TRUE) # converting ShelveLoc to an ordered factor
# you can tell ShelveLoc is now an ordered factor because of the '<' signs in Levels (Levels: Bad < Medium < Good)

lm_fit_ord <- lm(Sales ∼. + Income:Advertising + Price:Age, data = Carseats) # multiple regression model that includes all predictors plus some interaction terms
summary(lm_fit_ord)

contrasts(Carseats$ShelveLoc)

```

Ordered factors are automatically represented via orthogonal polynomials for linear regression. Now the intercept specifies the value of $y$ at the mean factor level; the $L$ (linear) parameter gives a measure of the linear trend ; $Q$ specifies the quadratic term (can also end up with cubic etc. terms). 

# 3.6.7 Writing Functions

```{r funct}
LoadLibraries <- function (){ # creating the function
  library (ISLR)
  library (MASS)
  print (" The libraries have been loaded .")
}

LoadLibraries # will print what is in the function.

LoadLibraries() # runs the function

```

