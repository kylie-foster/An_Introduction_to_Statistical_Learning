---
title: "An Introduction to Statistical Learning: Chapter 5 Labs"
author: "Kylie Foster"
date: "6 May 2019"
output: 
  prettydoc::html_pretty:
    theme: leonids
    highlight: github

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("ISLR")
library(MASS) # for lda, and contains a large collection of datasets
#library(GGally) # for ggpairs
library(tidyverse) # loads packages including ggplot2, dplyr
#library(skimr) # for nicer summaries using skim
#library(cowplot) # for plot_grid
#library(car) # for vif()
#library(olsrr) # for studentized residual plots
library(knitr) # for kable
#library(ggplot2)
library (class) # for knn()
library(ISLR) # data sets for the ISLR book
library(boot) # for cv.glm()
```

## 5.3.1 The Validation Set Approach


```{r sample}

set.seed (1)
train <- sample(392, 196)
# returns a vector of 196 random values from the range 1 to 392 (without replacement).

lm_fit <- lm(mpg ∼ horsepower, data=Auto, subset = train) # using the subset option in 
# lm() to fit a linear regression using only the observations corresponding to the 
# training set

predict_all <- predict(lm_fit, Auto) # using predict() to estimate the response for all 392
# observations
attach (Auto)
# estimated test MSE for the linear regression fit:
mean((mpg - predict_all)[-train]^2) 
# this calculates the MSE of the
# 196 observations in the validation set. (takes the difference between 
# predicted and actual, squares this difference and then finds the mean 
# - it is the final step that results in a single number rather than a vector). 
# The -train index selects only the observations that are not in the training set.

```

```{r quad_cubic}
# quadratic regression (fits horsepower + horsepower^2)
lm_fit2 <- lm(mpg ∼ poly(horsepower, 2), data = Auto, subset = train)
mean((mpg - predict(lm_fit2, Auto))[-train]^2) # test MSE for quadratic regression

# cubic regression (fits horsepower + horsepower^2 + horsepower^3)
lm_fit3 <- lm(mpg ∼ poly(horsepower, 3), data = Auto, subset = train)
mean((mpg -predict (lm_fit3 ,Auto))[-train ]^2) # test MSE for cubic regression

```

```{r different_train}

set.seed(2) # using a different seed to get a different training/test split
train <- sample(392, 196)
# linear regression
lm_fit <- lm(mpg ∼ horsepower, subset = train)
mean((mpg - predict(lm_fit, Auto))[-train ]^2) # test MSE

# quadratic regression
lm_fit2 <- lm(mpg ∼ poly(horsepower, 2), data = Auto, subset = train)
mean((mpg - predict(lm_fit2, Auto))[-train ]^2) # test MSE

# cubic regression
lm_fit3 <- lm(mpg ∼ poly(horsepower, 3), data = Auto, subset = train)
mean((mpg - predict(lm_fit3, Auto))[-train ]^2) # test MSE

```

Although the actual MSE values are slightly different, the results are consistent for two different seed values: a model that
predicts `mpg` using a quadratic function of `horsepower` performs better than
a model that involves only a linear function of `horsepower`, and there is
little evidence in favor of a model that uses a cubic function of `horsepower`.

## 5.3.2 Leave-One-Out Cross-Validation

```{r LOOCV}
glm_fit <- glm(mpg ∼ horsepower, data = Auto) # using glm to fit a linear regression model
coef(glm_fit)

# identical results are obtained using lm()
lm_fit <- lm(mpg ∼ horsepower, data = Auto)
coef(lm_fit)

glm_fit <- glm(mpg ∼ horsepower, data = Auto)
cv_err <- cv.glm(Auto, glm_fit)
cv_err$delta

```

In this case (LOOCV) both values of `delta` correspond to: 
$CV_{(n)} = \frac{1}{n}\Sigma^n_{i=1} MSE_i$

This is the LOOCV estimate for the test MSE, which is the average of test errors for each of the `n` model fits.

```{r LOOCV_poly}
cv_error <- rep (0,5) #initialising vector
for (i in 1:5){
glm_fit <- glm(mpg ∼ poly(horsepower, i), data = Auto) # iteratively fits polynomial 
# regressions for polynomials of order i = 1 to i = 5
cv_error[i] <- cv.glm(Auto, glm_fit)$delta[1] # computes the LOOCV cross-validation error,
# and stores it in the ith element of the vector cv_error
}
cv_error
```

## 5.3.3 k-Fold Cross-Validation

```{r k_fold}
set.seed(17)
cv_error_10 <- rep(0, 10) # initialising vector
for (i in 1:10) {
  glm_fit <- glm(mpg ∼ poly(horsepower, i), data = Auto)# iteratively fits polynomial 
# regressions for polynomials of order i = 1 to i = 5
  cv_error_10[i] <- cv.glm (Auto, glm_fit, K=10)$delta[1] # 10-fold cross-validation
  }
cv_error_10
```

When LOOCV is performed the two numbers associated with delta are
essentially the same. When we instead perform k-fold CV, then the two numbers associated with delta differ slightly. The first is the standard k-fold CV estimate, $CV_{(k)} = \frac{1}{k}\Sigma^k_{i=1}MSE_i$. The second is a bias corrected
version.

## 5.3.4 The Bootstrap

Performing a bootstrap analysis in `R` involves two steps:

1. create a function that computes the statistic of interest.

2. use the `boot()` function to perform the bootstrap by repeatedly sampling observations from the data
set with replacement.

#### Estimating the Accuracy of a Statistic of Interest

```{r boot_function}

alpha_fn <- function(data, index){# function takes as input the (X, Y) 
  # data as well as a vector indicating which observations should be 
  # used to estimate α
  X <- data$X[index]
  Y <- data$Y[index]
  return((var(Y) - cov(X,Y))/(var(X) + var(Y) -2*cov(X,Y))) # alpha
  }

alpha_fn(Portfolio, 1:100) # estimates α using all 100 observations
# the Portfolio data set is in the ISLR package

set.seed (1)
alpha_fn(Portfolio, sample(100, 100, replace = T))
# using sample() to randomly select 100 observations from the range 
# 1 to 100, with replacement and then recomputing alpha hat using 
# the new bootstrap data set.

boot(Portfolio, alpha_fn, R = 1000)

```

#### Estimating the Accuracy of a Linear Regression Model

```{r}
boot_fn <- function(data, index){
  return(coef(lm(mpg ∼ horsepower, data = data, subset = index)))
}
+ 
boot_fn(Auto, 1:392)
```

