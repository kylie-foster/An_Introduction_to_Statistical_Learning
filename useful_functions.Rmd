---
title: "Useful Functions"
author: "Kylie Foster"
date: "1 May 2019"
output: 
  prettydoc::html_pretty:
    theme: leonids
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(GGally) # for ggpairs
library(tidyverse) # loads packages including ggplot2
```

---

# ISL Chapter 2

### `ggpairs()`

Useful for exploratory data analysis. Produces a pairwise comparison of multivariate data. By default, `ggpairs` provides two different comparisons of each pair of columns and displays either the density (for continous variables) or count (for categorical variables) along the diagonal.

To carry out a pairwise comparison of all variables in a dataframe:

```{r ggpairs_all, message=FALSE}
data(tips, package = "reshape")
(pm <- ggpairs(tips))
```

If this creates too many subplots you can restrict the plot to a specific number of columns using:

```{r ggpairs_restrict, message=FALSE}

(pm <- ggpairs(tips, columns = c("total_bill", "time", "tip")))
# or equivalently you can use column numbers instead of variable names:
pm <- ggpairs(tips, columns = c(1, 6, 2))
```

To change the labels on the axes use `columnLabels`:

```{r ggpairs_labels, message=FALSE}
(pm <- ggpairs(tips, columns = c("total_bill", "time", "tip"), columnLabels = c("Total Bill", "Time of Day", "Tip")))

```

To apply aesthetics to all of the subplots (e.g. colour) use the `mapping` parameter:

```{r ggpairs_color, message=FALSE}
(pm <- ggpairs(tips, mapping = aes(color = sex), columns = c("total_bill", "time", "tip")))
```

It is also possible to change the `theme`, for example:

```{r ggpairs_theme, message=FALSE}
(pm <- ggpairs(tips, mapping = aes(color = sex), columns = c("total_bill", "time", "tip")) +
   theme_bw())
```

For more information (including how to replace default plots with different types of plots, add trendlines etc):
https://mran.microsoft.com/snapshot/2016-01-07/web/packages/GGally/vignettes/ggpairs.html

### `glimpse()`

This is like a transposed version of print: columns run down the page, and data runs across. This is more useful than the default display of tibbles because it makes it possible to see every column in a data frame/tibble. The width of the displayed data defaults to the width of the screen.

Example:

```{r}
glimpse(mtcars)
```

Source: Rdocumentation

- `summarise_if`
- `slice`
- `gather` sometimes required to use facet_wrap
- `case_when`

### Other notes:

- To suppress messages from readr (when using `read_csv`) in Rmarkdown output, use `options(readr.num_columns = 0)`.

- Use `message = FALSE` to prevent Rmarkdown output from displaying messages such as "`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."

- One way to check for missing values is to use: `sum(is.na(data))`. This will return the number of missing values.

- You can use `fig.height =` and `fig.width =` in the heading of a code chunk to change the dimensions of all figures in that chunk in the Rmarkdown output.

- use `as_tibble()` instead of `as.tibble()`

---

# ISL Chapter 3

## `lm()`

`lm(y ∼ x, data = data)`: fits `x`

`lm(y ∼ x1 + x2, data = data)`: fits `x2` and `x1`

`lm(y ∼., data = data)`: fits all variables in `data`

`lm(y ∼. -x1, data = data)`: fits all variables in `data` except `x1`

`lm(y ~ x1:x2, data = data)`: includes the interaction term `x1` x `x2` as a predictor.

`lm(y ~ x1*x2, data = data)`: includes `x1`, `x2` and the interaction term `x1` x `x2`; it is a shorthand for
`lstat` + `age` + `lstat:age` 

`lm(y ~ poly(x, 5), data = data)`: fits `x`, `x^2`, `x^3`, `x^4` and `x^5`.

`lm(y ~ .^2)`: creates all combinations of two-way interactions. Does NOT square the terms.

`lm(y ∼ x + 0)`: regression without an intercept term.

- `predict()`

- `geom_smooth()`

- `residuals()`

- `rstudent()`

- `which.max()`: identifies the index of the largest element of a vector.

- `hatvalues()`: need to find out what this does.

- `I()`

- `contrasts()`

- `factor(, ordered=TRUE)`

## `confit`

Computes confidence intervals for one or more parameters in a fitted model. 

`confint(object, parm, level = 0.95, …)`

Arguments/Inputs:

- object: a fitted model object.

- parm: a specification of which parameters are to be given confidence intervals, either a vector of numbers or a vector of names. If missing, all parameters are considered.

- level: the confidence level required (0.95 is the default).

Value/Output:

A matrix (or vector) with columns giving lower and upper confidence limits for each parameter.

---

# ISL Chapter 4

- `glm()`

If we use `glm()` to fit a model without passing in the `family` argument, then it performs linear regression,
just like the `lm()` function.

- `predict()`

- `lda()`. Used to fit a Linear Discriminant Analysis model. Part of the `MASS` library. The syntax is identical to that of `lm()`. LDA assumes that the variables are continuous, so it is not a good idea to use it when you have categorical variables (use logistic regression instead).

- `qda()`. Used to fit a Quadratic Discriminant Analysis model. Part of the `MASS` library. Syntax is identical to that of `lda()`. Again, discriminant analysis assumes the variables are continous, so it is not a good idea to use it when you have categorical variables.

## `knn()`. 
Used to fit K-nearest neighbors. Part of the `class` library. This function works differently to the other model-fitting
functions described above. Instead of a two-step
approach in which we first fit the model and then we use the model to make
predictions, `knn()` fits the model and forms predictions using a single command. The function requires four inputs.

1. A matrix containing the predictors associated with the **training data**.
2. A matrix containing the predictors associated with the data for which
we wish to make predictions (the **testing data**).
3. A vector containing the **class labels for the training observations**.
4. A value for **K, the number of nearest neighbors** to be used by the
classifier.

- `sample()`

### Other notes

- How to combine violin and boxplots:

`ggplot(Auto_wrap, aes(y = values, x = mpg01)) +
    geom_violin() +
    geom_boxplot(width=0.1) +
    facet_wrap(~ vars, scales = "free_y", ncol = 2)`
    
---
    
# ISL Chapter 5

## `sample()`

`sample(x, size, replace = FALSE, prob = NULL)`

`sample()` generates a sample of size `size` from the data frame or vector `x`, either with or without replacement.

Inputs:

- `x`: either a vector of one or more elements from which to choose, or a positive integer.

- `size`:	size of the sample to choose.

- `replace`: should sampling be with replacement? (TRUE or FALSE)

- `prob`:	probability weights for obtaining the elements of the vector being sampled

It's a good idea to use `set.seed()` before using `sample()` for reproducibility.

No package required (this is a base R package).

Sources: 

- http://www.datasciencemadesimple.com/sample-function-in-r/

- https://www.rdocumentation.org/packages/base/versions/3.5.3/topics/sample

## `cv.glm()`

`cv.glm(data, glmfit, cost, K)`

Calculates the estimated K-fold cross-validation prediction error for generalized linear models.

### Inputs:

- `data`: A matrix or data frame containing the data. The rows should be cases and the columns correspond to variables, one of which is the response.

- `glmfit`: An object of class "glm" containing the results of a generalized linear model fitted to data (using `glm()`).

- `cost`: A function of two vector arguments specifying the cost function for the cross-validation. The first argument to `cost` should correspond to the observed responses and the second argument should correspond to the predicted or fitted responses from the generalized linear model. `cost` must return a non-negative scalar value. **The default is the average squared error function.**

- `K`: The number of groups into which the data should be split to estimate the cross-validation prediction error. `K = 10` is a common choice. The value of `K` must be such that all groups are of approximately equal size. If the supplied value of `K` does not satisfy this criterion then it will be set to the closest integer which does and a warning is generated specifying the value of `K` used. **The default is to set `K` equal to the number of observations in data which gives the usual leave-one-out cross-validation.**

### Outputs

The returned value is a list with the following components:

- `call`: the original call to `cv.glm`.

- `K`: the value of `K` used for the K-fold cross validation. 

- `delta`: a vector of length two. The first component is the raw cross-validation estimate of prediction error. The second component is the adjusted cross-validation estimate. The adjustment is designed to compensate for the bias introduced by not using leave-one-out cross-validation.

When the default `cost` is used and Leave-One-Out Cross-Validation is performed the two numbers associated with `delta` are
essentially the same and correspond to: correspond to: 
$CV_{(n)} = \frac{1}{n}\Sigma^n_{i=1} MSE_i$ (where n is the number of rows in the data set and therefore the number of models). When the default `cost` is used and we instead perform k-fold CV, then the two numbers associated with delta differ slightly. The first is the standard k-fold CV estimate: $CV_{(k)} = \frac{1}{k}\Sigma^k_{i=1}MSE_i$. The second is a bias corrected
version.

- `seed`: the value of `.Random.seed` when `cv.glm` was called.

Package: `boot`

Sources:

- https://stat.ethz.ch/R-manual/R-devel/library/boot/html/cv.glm.html

- ISL

## boot()

