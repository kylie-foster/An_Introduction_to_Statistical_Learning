---
title: "An Introduction to Statistical Learning: Chapter 8 Applied Exercises"
author: "Kylie Foster"
date: "22 May 2019"
output: 
  prettydoc::html_pretty:
    theme: leonids
    highlight: github
---


```{r setup, include=FALSE}
library(tidyverse) # loads packages including ggplot2, dplyr
library(tree) # for classification and regression trees
library(ISLR) # data sets for the ISLR book. Contains the Carseats data set

knitr::opts_chunk$set(echo = TRUE)

```

## 8.3.1 Fitting Classification Trees

```{r loading_data}
# Using the Carseats data set from the ISLR library

# creating binary variable that is Yes if Sales exceeds 8
Carseats <- mutate(Carseats, High = as.factor(ifelse(Sales <= 8, "No", "Yes"))) # creating binary variable that i sYes if Sales exceeds 8

# Using the tree() function to fit a classification tree to predict High using all variables except Sales
tree_carseats <- tree(High ~. - Sales, Carseats)

# summary() listst the variables that are used as internal nodes, the number of terminal noede, and the training error.
summary(tree_carseats)
```

The training error rate is 9%.

A small deviance indicates a tree that provides a good fit to the training data.

```{r plot_tree}

plot(tree_carseats) # plotting tree
text(tree_carseats, pretty = 0) # displays the node labels, 
# pretty = 0 ensures category names for qualitative predictors are included.


```

The most important indicator of `Sales` appears to be shelving location,
since the first branch differentiates `Good` locations from `Bad` and `Medium`
locations.

If we just type the name of the tree object, R prints output corresponding
to each branch of the tree:

- the split criterion 

- the number of observations in that branch

- the deviance

- the overall prediction for the branch (`Yes` or `No`)

- the fraction of observations in that branch that take on values of `Yes` and `No`. 

- Branches that lead to terminal nodes areindicated using asterisks.

```{r}
tree_carseats
```

```{r}
set.seed(2, sample.kind = "Rounding") # the second term is needed in newer version of 
# R to reproduce the results in the textbook due to changes to set.seed and its use 
# with sample()
train <- sample(1:nrow(Carseats), 200) # vector specifying the index to use 
# to select 200 observations for the training set.
Carseats_test <- Carseats[-train, ] # selecting test set
tree_carseats <- tree(High ~. -Sales, Carseats, subset = train) # building tree model
tree_pred <- predict(tree_carseats, Carseats_test, type = "class") # finding predictions on test set.
# use argument type = "class" to tell R to return the actual class prediction 
table(tree_pred, Carseats_test$High) # confusion matrix
```

```{r}
set.seed(3)
cv_carseats <- cv.tree(tree_carseats, FUN = prune.misclass) # tree_carseats is the tree object above
# use the argument FUN=prune.misclass in order to indicate that we want the
# classification error rate to guide the cv and pruning process,
# rather than the default, deviance
names(cv_carseats)
cv_carseats

# plotting error rate as a function of both size and k
par(mfrow = c(1,2))
plot(cv_carseats$size, cv_carseats$dev, type = "b")
plot(cv_carseats$k, cv_carseats$dev, type = "b")
```

